{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's Tree Huggin' Time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before you begin:\n",
    "* This notebook will establish a MySQL connection, create a database, its\n",
    "associated tables, without the use of the MySQL UI. Please make sure to \n",
    "have mysqlclient installed by running \n",
    "```pip install mysqlclient``` \n",
    "in your python runtime environment.\n",
    "* The data set for NYC Trees is rather large and is not stored in this repository, please go to [Kaggle](https://www.kaggle.com/new-york-city/ny-2015-street-tree-census-tree-data/version/12) and download to the resources folder within this repository.\n",
    "* Remember to insert you password in mysql_scr.py (please do not alter the other variables)\n",
    "* <b>Warning!</b> The create_db variable will drop and create a database on use!\n",
    "* Let's get tree huggin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import mysql_scr\n",
    "import numpy as np\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import func\n",
    "import os\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up mysql initial connections\n",
    "init_string = (f\"root:{mysql_scr.pw}@localhost\")\n",
    "pre_engine = create_engine(f\"mysql://{init_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mysql db\n",
    "pre_engine.execute(f\"{mysql_scr.create_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize new mysql db connection\n",
    "engine = create_engine(f\"mysql://{init_string}/tree_db\")\n",
    "engine.execute(f\"{mysql_scr.create_tbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automap base and check if tables exist...\n",
    "Base = automap_base()\n",
    "Base.prepare(engine, reflect=True)\n",
    "Base.classes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file\n",
    "file = os.path.join(\"Resources\",\"2015-street-tree-census-tree-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data = pd.read_csv(file)\n",
    "tree_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain desired columns\n",
    "tree_data = tree_data[['postcode','spc_common','spc_latin','tree_dbh']]\n",
    "#clean data\n",
    "tree_data_drop = tree_data.dropna()\n",
    "tree_data_clean = tree_data_drop.drop_duplicates()\n",
    "\n",
    "tree_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain average tree diameter\n",
    "tree_info = tree_data_clean.groupby(['postcode','spc_common'])\n",
    "tree_diam = pd.DataFrame(tree_info['tree_dbh'].mean())\n",
    "\n",
    "#obtain number of tree type in postcode\n",
    "\n",
    "tree_quantity = pd.DataFrame(tree_data_clean.groupby(['postcode','spc_common']).size())\n",
    "\n",
    "tree_summary = tree_quantity.join(tree_diam).reset_index()\n",
    "tree_summary = tree_summary.rename(columns={0:'tree_count'})\n",
    "tree_summary.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns to match database table\n",
    "tree_transformed = tree_summary.rename(columns = {'postcode':'zip_code',\n",
    "                              'spc_common': 'species_nm',\n",
    "                              'tree_count': 'count_tree',\n",
    "                              'tree_dbh': 'avg_diameter'})\n",
    "tree_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe to database\n",
    "tree_transformed.to_sql(name='nyc_tree',con=engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate import\n",
    "result = engine.execute('Select * from nyc_tree limit 10')\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sp_df = tree_data_clean['spc_common']\n",
    "tree_sp_unique = pd.DataFrame(tree_sp_df.drop_duplicates())\n",
    "#test_df_unique.str.strip()\n",
    "\n",
    "tree_sp_unique['spc_common'] = tree_sp_unique['spc_common'].str.replace(\" \",\"\")\n",
    "tree_sp_ins = tree_sp_unique.rename(columns={'spc_common':'web_common_nm'})\n",
    "tree_sp_ins['species_nm'] = tree_data_clean['spc_common']\n",
    "tree_sp_ins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sp_ins.to_sql(name='tree_species',con=engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate import\n",
    "result = engine.execute('Select * from tree_species limit 20')\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize base\n",
    "Tree = Base.classes.tree_species\n",
    "session = Session(engine)\n",
    "Tree_species = session.query(Tree.species_nm).all()\n",
    "\n",
    "# declare tree_list variable and append from database table\n",
    "tree_list = []\n",
    "\n",
    "for tree in Tree_species:\n",
    "    tree_list.append(tree.species_nm)\n",
    "    \n",
    "print(tree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create webkey (will be replaced) for looking up to website\n",
    "trees  = [x.replace(' ','').lower() for x in tree_list]\n",
    "\n",
    "for i in trees:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Please Change/ Un-comment the executable path based on your OS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open brower connection\n",
    "\n",
    "# For Mac Users:\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "\n",
    "# For Windows Users:\n",
    "# executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare url and parser\n",
    "url = \"http://leafsnap.com/species/\"\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raking! Get it? Because its trees!\n",
    "(if you don't like that joke, you can leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get webscraping!\n",
    "# Get div data related to the first species tab\n",
    "ne_species = soup.find('div', id='species-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all tr rows to loop through the data\n",
    "species = ne_species.table.tbody.find_all('tr')\n",
    "\n",
    "for td in species:\n",
    "    img = td.find_all('img')\n",
    "    name = td.find_all('td')\n",
    "    link = td.find_all('a')\n",
    "    qry_name = name[3].text.replace(\" \", \"\").lower()\n",
    "    \n",
    "    #check to see if web tree is in sql tree\n",
    "    if qry_name in trees:\n",
    "        image_link = img[0]['src']\n",
    "        web_name = name[3].text\n",
    "        print(image_link)\n",
    "        print(name[3].text)\n",
    "        print(link[0]['href'])\n",
    "        \n",
    "        ## update sql table\n",
    "        session.query(Tree.img_loc)\\\n",
    "                .filter(func.lower(func.replace(Tree.species_nm,' ','')) == qry_name)\\\n",
    "                .update({\"img_loc\":image_link}, synchronize_session='fetch')\n",
    "        session.commit()\n",
    "\n",
    "        ## go to tree data page\n",
    "        browser.click_link_by_partial_href(link[0]['href'])\n",
    "        \n",
    "        #get new webpage html and parse\n",
    "        html_species = browser.html\n",
    "        soup_species = bs(html_species, \"html.parser\")\n",
    "        \n",
    "        tree = soup_species.find_all('dd')\n",
    "        \n",
    "        try:\n",
    "            habitat = tree[0].text\n",
    "        except IndexError:\n",
    "            habitat = 'N/A'\n",
    "        try:\n",
    "            growth = tree[1].text\n",
    "        except IndexError:\n",
    "            growth = 'N/A'\n",
    "        try:\n",
    "            bloom = tree[2].text\n",
    "        except IndexError:\n",
    "            bloom = \"N/A\"\n",
    "        try:\n",
    "            longevity = tree[3].text\n",
    "        except IndexError:\n",
    "            longevity = 'N/A'\n",
    "        \n",
    "        session.query(Tree.habitat, Tree.growth_habit, Tree.bloom_time, Tree.longevity )\\\n",
    "                .filter(func.lower(func.replace(Tree.species_nm,' ','')) == qry_name)\\\n",
    "                .update({\"habitat\":habitat, \"growth_habit\": growth, \"bloom_time\": bloom, \"longevity\": longevity}, synchronize_session='fetch')\n",
    "\n",
    "        session.commit()\n",
    "        print(habitat, growth, bloom, longevity)\n",
    " \n",
    "        browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = engine.execute('select * from tree_species limit 10')\n",
    "for s in spec:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
